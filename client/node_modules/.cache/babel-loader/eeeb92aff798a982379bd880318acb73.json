{"ast":null,"code":"'use strict';\n\nObject.defineProperty(exports, '__esModule', {\n  value: true\n});\nvar common = require('./common.js');\nvar token = require('./token.js');\nvar jump = require('./jump.js');\nconst defaultDecodeOptions = {\n  strict: false,\n  allowIndefinite: true,\n  allowUndefined: true,\n  allowBigInt: true\n};\nclass Tokeniser {\n  constructor(data) {\n    let options = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    this.pos = 0;\n    this.data = data;\n    this.options = options;\n  }\n  done() {\n    return this.pos >= this.data.length;\n  }\n  next() {\n    const byt = this.data[this.pos];\n    let token = jump.quick[byt];\n    if (token === undefined) {\n      const decoder = jump.jump[byt];\n      if (!decoder) {\n        throw new Error(\"\".concat(common.decodeErrPrefix, \" no decoder for major type \").concat(byt >>> 5, \" (byte 0x\").concat(byt.toString(16).padStart(2, '0'), \")\"));\n      }\n      const minor = byt & 31;\n      token = decoder(this.data, this.pos, minor, this.options);\n    }\n    this.pos += token.encodedLength;\n    return token;\n  }\n}\nconst DONE = Symbol.for('DONE');\nconst BREAK = Symbol.for('BREAK');\nfunction tokenToArray(token, tokeniser, options) {\n  const arr = [];\n  for (let i = 0; i < token.value; i++) {\n    const value = tokensToObject(tokeniser, options);\n    if (value === BREAK) {\n      if (token.value === Infinity) {\n        break;\n      }\n      throw new Error(\"\".concat(common.decodeErrPrefix, \" got unexpected break to lengthed array\"));\n    }\n    if (value === DONE) {\n      throw new Error(\"\".concat(common.decodeErrPrefix, \" found array but not enough entries (got \").concat(i, \", expected \").concat(token.value, \")\"));\n    }\n    arr[i] = value;\n  }\n  return arr;\n}\nfunction tokenToMap(token, tokeniser, options) {\n  const useMaps = options.useMaps === true;\n  const obj = useMaps ? undefined : {};\n  const m = useMaps ? new Map() : undefined;\n  for (let i = 0; i < token.value; i++) {\n    const key = tokensToObject(tokeniser, options);\n    if (key === BREAK) {\n      if (token.value === Infinity) {\n        break;\n      }\n      throw new Error(\"\".concat(common.decodeErrPrefix, \" got unexpected break to lengthed map\"));\n    }\n    if (key === DONE) {\n      throw new Error(\"\".concat(common.decodeErrPrefix, \" found map but not enough entries (got \").concat(i, \" [no key], expected \").concat(token.value, \")\"));\n    }\n    if (useMaps !== true && typeof key !== 'string') {\n      throw new Error(\"\".concat(common.decodeErrPrefix, \" non-string keys not supported (got \").concat(typeof key, \")\"));\n    }\n    if (options.rejectDuplicateMapKeys === true) {\n      if (useMaps && m.has(key) || !useMaps && key in obj) {\n        throw new Error(\"\".concat(common.decodeErrPrefix, \" found repeat map key \\\"\").concat(key, \"\\\"\"));\n      }\n    }\n    const value = tokensToObject(tokeniser, options);\n    if (value === DONE) {\n      throw new Error(\"\".concat(common.decodeErrPrefix, \" found map but not enough entries (got \").concat(i, \" [no value], expected \").concat(token.value, \")\"));\n    }\n    if (useMaps) {\n      m.set(key, value);\n    } else {\n      obj[key] = value;\n    }\n  }\n  return useMaps ? m : obj;\n}\nfunction tokensToObject(tokeniser, options) {\n  if (tokeniser.done()) {\n    return DONE;\n  }\n  const token$1 = tokeniser.next();\n  if (token$1.type === token.Type.break) {\n    return BREAK;\n  }\n  if (token$1.type.terminal) {\n    return token$1.value;\n  }\n  if (token$1.type === token.Type.array) {\n    return tokenToArray(token$1, tokeniser, options);\n  }\n  if (token$1.type === token.Type.map) {\n    return tokenToMap(token$1, tokeniser, options);\n  }\n  if (token$1.type === token.Type.tag) {\n    if (options.tags && typeof options.tags[token$1.value] === 'function') {\n      const tagged = tokensToObject(tokeniser, options);\n      return options.tags[token$1.value](tagged);\n    }\n    throw new Error(\"\".concat(common.decodeErrPrefix, \" tag not supported (\").concat(token$1.value, \")\"));\n  }\n  throw new Error('unsupported');\n}\nfunction decode(data, options) {\n  if (!(data instanceof Uint8Array)) {\n    throw new Error(\"\".concat(common.decodeErrPrefix, \" data to decode must be a Uint8Array\"));\n  }\n  options = Object.assign({}, defaultDecodeOptions, options);\n  const tokeniser = options.tokenizer || new Tokeniser(data, options);\n  const decoded = tokensToObject(tokeniser, options);\n  if (decoded === DONE) {\n    throw new Error(\"\".concat(common.decodeErrPrefix, \" did not find any content to decode\"));\n  }\n  if (decoded === BREAK) {\n    throw new Error(\"\".concat(common.decodeErrPrefix, \" got unexpected break\"));\n  }\n  if (!tokeniser.done()) {\n    throw new Error(\"\".concat(common.decodeErrPrefix, \" too many terminals, data makes no sense\"));\n  }\n  return decoded;\n}\nexports.Tokeniser = Tokeniser;\nexports.decode = decode;\nexports.tokensToObject = tokensToObject;","map":{"version":3,"names":["Object","defineProperty","exports","value","common","require","token","jump","defaultDecodeOptions","strict","allowIndefinite","allowUndefined","allowBigInt","Tokeniser","constructor","data","options","arguments","length","undefined","pos","done","next","byt","quick","decoder","Error","concat","decodeErrPrefix","toString","padStart","minor","encodedLength","DONE","Symbol","for","BREAK","tokenToArray","tokeniser","arr","i","tokensToObject","Infinity","tokenToMap","useMaps","obj","m","Map","key","rejectDuplicateMapKeys","has","set","token$1","type","Type","break","terminal","array","map","tag","tags","tagged","decode","Uint8Array","assign","tokenizer","decoded"],"sources":["C:/6th sem/bc/Crime-Records-Blockchain/client/node_modules/cborg/cjs/lib/decode.js"],"sourcesContent":["'use strict';\n\nObject.defineProperty(exports, '__esModule', { value: true });\n\nvar common = require('./common.js');\nvar token = require('./token.js');\nvar jump = require('./jump.js');\n\nconst defaultDecodeOptions = {\n  strict: false,\n  allowIndefinite: true,\n  allowUndefined: true,\n  allowBigInt: true\n};\nclass Tokeniser {\n  constructor(data, options = {}) {\n    this.pos = 0;\n    this.data = data;\n    this.options = options;\n  }\n  done() {\n    return this.pos >= this.data.length;\n  }\n  next() {\n    const byt = this.data[this.pos];\n    let token = jump.quick[byt];\n    if (token === undefined) {\n      const decoder = jump.jump[byt];\n      if (!decoder) {\n        throw new Error(`${ common.decodeErrPrefix } no decoder for major type ${ byt >>> 5 } (byte 0x${ byt.toString(16).padStart(2, '0') })`);\n      }\n      const minor = byt & 31;\n      token = decoder(this.data, this.pos, minor, this.options);\n    }\n    this.pos += token.encodedLength;\n    return token;\n  }\n}\nconst DONE = Symbol.for('DONE');\nconst BREAK = Symbol.for('BREAK');\nfunction tokenToArray(token, tokeniser, options) {\n  const arr = [];\n  for (let i = 0; i < token.value; i++) {\n    const value = tokensToObject(tokeniser, options);\n    if (value === BREAK) {\n      if (token.value === Infinity) {\n        break;\n      }\n      throw new Error(`${ common.decodeErrPrefix } got unexpected break to lengthed array`);\n    }\n    if (value === DONE) {\n      throw new Error(`${ common.decodeErrPrefix } found array but not enough entries (got ${ i }, expected ${ token.value })`);\n    }\n    arr[i] = value;\n  }\n  return arr;\n}\nfunction tokenToMap(token, tokeniser, options) {\n  const useMaps = options.useMaps === true;\n  const obj = useMaps ? undefined : {};\n  const m = useMaps ? new Map() : undefined;\n  for (let i = 0; i < token.value; i++) {\n    const key = tokensToObject(tokeniser, options);\n    if (key === BREAK) {\n      if (token.value === Infinity) {\n        break;\n      }\n      throw new Error(`${ common.decodeErrPrefix } got unexpected break to lengthed map`);\n    }\n    if (key === DONE) {\n      throw new Error(`${ common.decodeErrPrefix } found map but not enough entries (got ${ i } [no key], expected ${ token.value })`);\n    }\n    if (useMaps !== true && typeof key !== 'string') {\n      throw new Error(`${ common.decodeErrPrefix } non-string keys not supported (got ${ typeof key })`);\n    }\n    if (options.rejectDuplicateMapKeys === true) {\n      if (useMaps && m.has(key) || !useMaps && key in obj) {\n        throw new Error(`${ common.decodeErrPrefix } found repeat map key \"${ key }\"`);\n      }\n    }\n    const value = tokensToObject(tokeniser, options);\n    if (value === DONE) {\n      throw new Error(`${ common.decodeErrPrefix } found map but not enough entries (got ${ i } [no value], expected ${ token.value })`);\n    }\n    if (useMaps) {\n      m.set(key, value);\n    } else {\n      obj[key] = value;\n    }\n  }\n  return useMaps ? m : obj;\n}\nfunction tokensToObject(tokeniser, options) {\n  if (tokeniser.done()) {\n    return DONE;\n  }\n  const token$1 = tokeniser.next();\n  if (token$1.type === token.Type.break) {\n    return BREAK;\n  }\n  if (token$1.type.terminal) {\n    return token$1.value;\n  }\n  if (token$1.type === token.Type.array) {\n    return tokenToArray(token$1, tokeniser, options);\n  }\n  if (token$1.type === token.Type.map) {\n    return tokenToMap(token$1, tokeniser, options);\n  }\n  if (token$1.type === token.Type.tag) {\n    if (options.tags && typeof options.tags[token$1.value] === 'function') {\n      const tagged = tokensToObject(tokeniser, options);\n      return options.tags[token$1.value](tagged);\n    }\n    throw new Error(`${ common.decodeErrPrefix } tag not supported (${ token$1.value })`);\n  }\n  throw new Error('unsupported');\n}\nfunction decode(data, options) {\n  if (!(data instanceof Uint8Array)) {\n    throw new Error(`${ common.decodeErrPrefix } data to decode must be a Uint8Array`);\n  }\n  options = Object.assign({}, defaultDecodeOptions, options);\n  const tokeniser = options.tokenizer || new Tokeniser(data, options);\n  const decoded = tokensToObject(tokeniser, options);\n  if (decoded === DONE) {\n    throw new Error(`${ common.decodeErrPrefix } did not find any content to decode`);\n  }\n  if (decoded === BREAK) {\n    throw new Error(`${ common.decodeErrPrefix } got unexpected break`);\n  }\n  if (!tokeniser.done()) {\n    throw new Error(`${ common.decodeErrPrefix } too many terminals, data makes no sense`);\n  }\n  return decoded;\n}\n\nexports.Tokeniser = Tokeniser;\nexports.decode = decode;\nexports.tokensToObject = tokensToObject;\n"],"mappings":"AAAA,YAAY;;AAEZA,MAAM,CAACC,cAAc,CAACC,OAAO,EAAE,YAAY,EAAE;EAAEC,KAAK,EAAE;AAAK,CAAC,CAAC;AAE7D,IAAIC,MAAM,GAAGC,OAAO,CAAC,aAAa,CAAC;AACnC,IAAIC,KAAK,GAAGD,OAAO,CAAC,YAAY,CAAC;AACjC,IAAIE,IAAI,GAAGF,OAAO,CAAC,WAAW,CAAC;AAE/B,MAAMG,oBAAoB,GAAG;EAC3BC,MAAM,EAAE,KAAK;EACbC,eAAe,EAAE,IAAI;EACrBC,cAAc,EAAE,IAAI;EACpBC,WAAW,EAAE;AACf,CAAC;AACD,MAAMC,SAAS,CAAC;EACdC,WAAWA,CAACC,IAAI,EAAgB;IAAA,IAAdC,OAAO,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,CAAC,CAAC;IAC5B,IAAI,CAACG,GAAG,GAAG,CAAC;IACZ,IAAI,CAACL,IAAI,GAAGA,IAAI;IAChB,IAAI,CAACC,OAAO,GAAGA,OAAO;EACxB;EACAK,IAAIA,CAAA,EAAG;IACL,OAAO,IAAI,CAACD,GAAG,IAAI,IAAI,CAACL,IAAI,CAACG,MAAM;EACrC;EACAI,IAAIA,CAAA,EAAG;IACL,MAAMC,GAAG,GAAG,IAAI,CAACR,IAAI,CAAC,IAAI,CAACK,GAAG,CAAC;IAC/B,IAAId,KAAK,GAAGC,IAAI,CAACiB,KAAK,CAACD,GAAG,CAAC;IAC3B,IAAIjB,KAAK,KAAKa,SAAS,EAAE;MACvB,MAAMM,OAAO,GAAGlB,IAAI,CAACA,IAAI,CAACgB,GAAG,CAAC;MAC9B,IAAI,CAACE,OAAO,EAAE;QACZ,MAAM,IAAIC,KAAK,IAAAC,MAAA,CAAKvB,MAAM,CAACwB,eAAe,iCAAAD,MAAA,CAAgCJ,GAAG,KAAK,CAAC,eAAAI,MAAA,CAAcJ,GAAG,CAACM,QAAQ,CAAC,EAAE,CAAC,CAACC,QAAQ,CAAC,CAAC,EAAE,GAAG,CAAC,MAAI,CAAC;MACzI;MACA,MAAMC,KAAK,GAAGR,GAAG,GAAG,EAAE;MACtBjB,KAAK,GAAGmB,OAAO,CAAC,IAAI,CAACV,IAAI,EAAE,IAAI,CAACK,GAAG,EAAEW,KAAK,EAAE,IAAI,CAACf,OAAO,CAAC;IAC3D;IACA,IAAI,CAACI,GAAG,IAAId,KAAK,CAAC0B,aAAa;IAC/B,OAAO1B,KAAK;EACd;AACF;AACA,MAAM2B,IAAI,GAAGC,MAAM,CAACC,GAAG,CAAC,MAAM,CAAC;AAC/B,MAAMC,KAAK,GAAGF,MAAM,CAACC,GAAG,CAAC,OAAO,CAAC;AACjC,SAASE,YAAYA,CAAC/B,KAAK,EAAEgC,SAAS,EAAEtB,OAAO,EAAE;EAC/C,MAAMuB,GAAG,GAAG,EAAE;EACd,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGlC,KAAK,CAACH,KAAK,EAAEqC,CAAC,EAAE,EAAE;IACpC,MAAMrC,KAAK,GAAGsC,cAAc,CAACH,SAAS,EAAEtB,OAAO,CAAC;IAChD,IAAIb,KAAK,KAAKiC,KAAK,EAAE;MACnB,IAAI9B,KAAK,CAACH,KAAK,KAAKuC,QAAQ,EAAE;QAC5B;MACF;MACA,MAAM,IAAIhB,KAAK,IAAAC,MAAA,CAAKvB,MAAM,CAACwB,eAAe,4CAA0C,CAAC;IACvF;IACA,IAAIzB,KAAK,KAAK8B,IAAI,EAAE;MAClB,MAAM,IAAIP,KAAK,IAAAC,MAAA,CAAKvB,MAAM,CAACwB,eAAe,+CAAAD,MAAA,CAA8Ca,CAAC,iBAAAb,MAAA,CAAgBrB,KAAK,CAACH,KAAK,MAAI,CAAC;IAC3H;IACAoC,GAAG,CAACC,CAAC,CAAC,GAAGrC,KAAK;EAChB;EACA,OAAOoC,GAAG;AACZ;AACA,SAASI,UAAUA,CAACrC,KAAK,EAAEgC,SAAS,EAAEtB,OAAO,EAAE;EAC7C,MAAM4B,OAAO,GAAG5B,OAAO,CAAC4B,OAAO,KAAK,IAAI;EACxC,MAAMC,GAAG,GAAGD,OAAO,GAAGzB,SAAS,GAAG,CAAC,CAAC;EACpC,MAAM2B,CAAC,GAAGF,OAAO,GAAG,IAAIG,GAAG,CAAC,CAAC,GAAG5B,SAAS;EACzC,KAAK,IAAIqB,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGlC,KAAK,CAACH,KAAK,EAAEqC,CAAC,EAAE,EAAE;IACpC,MAAMQ,GAAG,GAAGP,cAAc,CAACH,SAAS,EAAEtB,OAAO,CAAC;IAC9C,IAAIgC,GAAG,KAAKZ,KAAK,EAAE;MACjB,IAAI9B,KAAK,CAACH,KAAK,KAAKuC,QAAQ,EAAE;QAC5B;MACF;MACA,MAAM,IAAIhB,KAAK,IAAAC,MAAA,CAAKvB,MAAM,CAACwB,eAAe,0CAAwC,CAAC;IACrF;IACA,IAAIoB,GAAG,KAAKf,IAAI,EAAE;MAChB,MAAM,IAAIP,KAAK,IAAAC,MAAA,CAAKvB,MAAM,CAACwB,eAAe,6CAAAD,MAAA,CAA4Ca,CAAC,0BAAAb,MAAA,CAAyBrB,KAAK,CAACH,KAAK,MAAI,CAAC;IAClI;IACA,IAAIyC,OAAO,KAAK,IAAI,IAAI,OAAOI,GAAG,KAAK,QAAQ,EAAE;MAC/C,MAAM,IAAItB,KAAK,IAAAC,MAAA,CAAKvB,MAAM,CAACwB,eAAe,0CAAAD,MAAA,CAAyC,OAAOqB,GAAG,MAAI,CAAC;IACpG;IACA,IAAIhC,OAAO,CAACiC,sBAAsB,KAAK,IAAI,EAAE;MAC3C,IAAIL,OAAO,IAAIE,CAAC,CAACI,GAAG,CAACF,GAAG,CAAC,IAAI,CAACJ,OAAO,IAAII,GAAG,IAAIH,GAAG,EAAE;QACnD,MAAM,IAAInB,KAAK,IAAAC,MAAA,CAAKvB,MAAM,CAACwB,eAAe,8BAAAD,MAAA,CAA4BqB,GAAG,OAAI,CAAC;MAChF;IACF;IACA,MAAM7C,KAAK,GAAGsC,cAAc,CAACH,SAAS,EAAEtB,OAAO,CAAC;IAChD,IAAIb,KAAK,KAAK8B,IAAI,EAAE;MAClB,MAAM,IAAIP,KAAK,IAAAC,MAAA,CAAKvB,MAAM,CAACwB,eAAe,6CAAAD,MAAA,CAA4Ca,CAAC,4BAAAb,MAAA,CAA2BrB,KAAK,CAACH,KAAK,MAAI,CAAC;IACpI;IACA,IAAIyC,OAAO,EAAE;MACXE,CAAC,CAACK,GAAG,CAACH,GAAG,EAAE7C,KAAK,CAAC;IACnB,CAAC,MAAM;MACL0C,GAAG,CAACG,GAAG,CAAC,GAAG7C,KAAK;IAClB;EACF;EACA,OAAOyC,OAAO,GAAGE,CAAC,GAAGD,GAAG;AAC1B;AACA,SAASJ,cAAcA,CAACH,SAAS,EAAEtB,OAAO,EAAE;EAC1C,IAAIsB,SAAS,CAACjB,IAAI,CAAC,CAAC,EAAE;IACpB,OAAOY,IAAI;EACb;EACA,MAAMmB,OAAO,GAAGd,SAAS,CAAChB,IAAI,CAAC,CAAC;EAChC,IAAI8B,OAAO,CAACC,IAAI,KAAK/C,KAAK,CAACgD,IAAI,CAACC,KAAK,EAAE;IACrC,OAAOnB,KAAK;EACd;EACA,IAAIgB,OAAO,CAACC,IAAI,CAACG,QAAQ,EAAE;IACzB,OAAOJ,OAAO,CAACjD,KAAK;EACtB;EACA,IAAIiD,OAAO,CAACC,IAAI,KAAK/C,KAAK,CAACgD,IAAI,CAACG,KAAK,EAAE;IACrC,OAAOpB,YAAY,CAACe,OAAO,EAAEd,SAAS,EAAEtB,OAAO,CAAC;EAClD;EACA,IAAIoC,OAAO,CAACC,IAAI,KAAK/C,KAAK,CAACgD,IAAI,CAACI,GAAG,EAAE;IACnC,OAAOf,UAAU,CAACS,OAAO,EAAEd,SAAS,EAAEtB,OAAO,CAAC;EAChD;EACA,IAAIoC,OAAO,CAACC,IAAI,KAAK/C,KAAK,CAACgD,IAAI,CAACK,GAAG,EAAE;IACnC,IAAI3C,OAAO,CAAC4C,IAAI,IAAI,OAAO5C,OAAO,CAAC4C,IAAI,CAACR,OAAO,CAACjD,KAAK,CAAC,KAAK,UAAU,EAAE;MACrE,MAAM0D,MAAM,GAAGpB,cAAc,CAACH,SAAS,EAAEtB,OAAO,CAAC;MACjD,OAAOA,OAAO,CAAC4C,IAAI,CAACR,OAAO,CAACjD,KAAK,CAAC,CAAC0D,MAAM,CAAC;IAC5C;IACA,MAAM,IAAInC,KAAK,IAAAC,MAAA,CAAKvB,MAAM,CAACwB,eAAe,0BAAAD,MAAA,CAAyByB,OAAO,CAACjD,KAAK,MAAI,CAAC;EACvF;EACA,MAAM,IAAIuB,KAAK,CAAC,aAAa,CAAC;AAChC;AACA,SAASoC,MAAMA,CAAC/C,IAAI,EAAEC,OAAO,EAAE;EAC7B,IAAI,EAAED,IAAI,YAAYgD,UAAU,CAAC,EAAE;IACjC,MAAM,IAAIrC,KAAK,IAAAC,MAAA,CAAKvB,MAAM,CAACwB,eAAe,yCAAuC,CAAC;EACpF;EACAZ,OAAO,GAAGhB,MAAM,CAACgE,MAAM,CAAC,CAAC,CAAC,EAAExD,oBAAoB,EAAEQ,OAAO,CAAC;EAC1D,MAAMsB,SAAS,GAAGtB,OAAO,CAACiD,SAAS,IAAI,IAAIpD,SAAS,CAACE,IAAI,EAAEC,OAAO,CAAC;EACnE,MAAMkD,OAAO,GAAGzB,cAAc,CAACH,SAAS,EAAEtB,OAAO,CAAC;EAClD,IAAIkD,OAAO,KAAKjC,IAAI,EAAE;IACpB,MAAM,IAAIP,KAAK,IAAAC,MAAA,CAAKvB,MAAM,CAACwB,eAAe,wCAAsC,CAAC;EACnF;EACA,IAAIsC,OAAO,KAAK9B,KAAK,EAAE;IACrB,MAAM,IAAIV,KAAK,IAAAC,MAAA,CAAKvB,MAAM,CAACwB,eAAe,0BAAwB,CAAC;EACrE;EACA,IAAI,CAACU,SAAS,CAACjB,IAAI,CAAC,CAAC,EAAE;IACrB,MAAM,IAAIK,KAAK,IAAAC,MAAA,CAAKvB,MAAM,CAACwB,eAAe,6CAA2C,CAAC;EACxF;EACA,OAAOsC,OAAO;AAChB;AAEAhE,OAAO,CAACW,SAAS,GAAGA,SAAS;AAC7BX,OAAO,CAAC4D,MAAM,GAAGA,MAAM;AACvB5D,OAAO,CAACuC,cAAc,GAAGA,cAAc","ignoreList":[]},"metadata":{},"sourceType":"script"}